# Human Tasks:
# 1. Ensure S3 bucket is created and properly configured for log storage
# 2. Configure AWS credentials for S3 access
# 3. Verify network connectivity between Fluentd and other monitoring components
# 4. Review and adjust log retention periods based on compliance requirements
# 5. Set up appropriate authentication for log access

# Requirement: Log Aggregation and Monitoring (Technical Specification/7.4 Cross-Cutting Concerns/7.4.1 Monitoring and Observability)
# Provides a centralized system for collecting, processing, and forwarding logs to enhance observability and debugging.

# System-wide configurations
<system>
  log_level "#{ENV['LOG_LEVEL'] || 'info'}"
  workers 4
  root_dir /fluentd/log
</system>

# Source configurations
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Collect Docker container logs
<source>
  @type tail
  path /var/log/containers/*.log
  pos_file /fluentd/log/containers.log.pos
  tag kubernetes.*
  read_from_head true
  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Collect system logs
<source>
  @type tail
  path /var/log/syslog
  pos_file /fluentd/log/syslog.pos
  tag system.syslog
  <parse>
    @type syslog
  </parse>
</source>

# Filter configurations
<filter kubernetes.**>
  @type kubernetes_metadata
  kubernetes_url "#{ENV['KUBERNETES_URL']}"
  bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
  ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  skip_labels false
  skip_container_metadata false
  skip_namespace_metadata false
</filter>

# Add common labels
<filter **>
  @type record_transformer
  <record>
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
    hostname "#{Socket.gethostname}"
  </record>
</filter>

# Output configurations
# Forward logs to Loki
<match kubernetes.**>
  @type loki
  url "#{ENV['LOKI_URL'] || 'http://loki:3100'}"
  extra_labels {"job": "fluentd"}
  flush_interval 10s
  flush_at_shutdown true
  buffer_chunk_limit 1m
  <label>
    container $.kubernetes.container_name
    pod $.kubernetes.pod_name
    namespace $.kubernetes.namespace_name
  </label>
</match>

# Forward logs to Elasticsearch
<match system.**>
  @type elasticsearch
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  logstash_format true
  logstash_prefix system-logs
  flush_interval 5s
  retry_limit 10
  retry_wait 30
  <buffer>
    @type file
    path /fluentd/log/elasticsearch
    flush_mode interval
    retry_type exponential_backoff
    flush_interval 5s
    flush_thread_count 4
    overflow_action block
  </buffer>
</match>

# Archive logs to S3 for long-term storage
<match **>
  @type s3
  aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
  aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
  s3_bucket "#{ENV['S3_BUCKET_NAME']}"
  s3_region "#{ENV['AWS_REGION'] || 'us-east-1'}"
  path logs/%Y/%m/%d/
  s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
  time_slice_format %Y%m%d-%H
  <buffer time>
    @type file
    path /fluentd/log/s3
    timekey 3600
    timekey_wait 10m
    chunk_limit_size 256m
  </buffer>
  <format>
    @type json
  </format>
</match>

# Monitor Fluentd metrics
<source>
  @type monitor_agent
  bind 0.0.0.0
  port 24220
</source>

# Export metrics to Prometheus
<source>
  @type prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>

<source>
  @type prometheus_output_monitor
  interval 10
  <labels>
    hostname ${hostname}
  </labels>
</source>

# Error handling
<label @ERROR>
  <match **>
    @type file
    path /fluentd/log/error.log
    append true
    <buffer>
      @type file
      path /fluentd/log/error
      flush_mode interval
      retry_type exponential_backoff
      flush_interval 60s
      retry_forever false
      retry_max_times 5
    </buffer>
  </match>
</label>